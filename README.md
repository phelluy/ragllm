# ragllm
A short introduction to RAG for LLM

## Description

Ce d√©p√¥t contient un cours d'introduction aux techniques de RAG (Retrieval-Augmented Generation) pour les LLM, destin√© √† un niveau M1.

## Structure du projet

```
ragllm/
‚îú‚îÄ‚îÄ doc/                          # Documentation et slides
‚îÇ   ‚îî‚îÄ‚îÄ slides_rag.typ           # Slides Typst (30 diapos)
‚îú‚îÄ‚îÄ data/                         # Documents markdown pour indexation
‚îÇ   ‚îú‚îÄ‚îÄ introduction_rag.md      # Introduction au RAG
‚îÇ   ‚îú‚îÄ‚îÄ embeddings.md            # Les embeddings
‚îÇ   ‚îú‚îÄ‚îÄ bases_vectorielles.md    # Bases de donn√©es vectorielles
‚îÇ   ‚îú‚îÄ‚îÄ chunking.md              # Strat√©gies de d√©coupage
‚îÇ   ‚îî‚îÄ‚îÄ generation_llm.md        # G√©n√©ration avec LLM
‚îú‚îÄ‚îÄ data_big/                     # Documents volumineux (ex: romans)
‚îÇ   ‚îî‚îÄ‚îÄ chartreuse_de_parme_stendhal.md
‚îú‚îÄ‚îÄ llm_providers.py             # Configuration modulaire des providers LLM
‚îú‚îÄ‚îÄ rag_demo.py                  # D√©monstration RAG en Python (script)
‚îú‚îÄ‚îÄ rag_demo.ipynb               # Notebook Jupyter pour Colab
‚îî‚îÄ‚îÄ requirements.txt             # D√©pendances Python
```

## Installation

### Option 1 : Utilisation locale (script Python)

1. Cloner le d√©p√¥t :
```bash
git clone https://github.com/phelluy/ragllm.git
cd ragllm
```

2. Cr√©er un environnement virtuel (recommand√©) :
```bash
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate  # Windows
```

3. Installer les d√©pendances :
```bash
pip install -r requirements.txt
```

### Option 2 : Utilisation sur Google Colab (recommand√© pour d√©butants)

Le notebook `rag_demo.ipynb` est pr√™t pour Google Colab et ne n√©cessite aucune installation locale.

**√âtapes rapides :**
1. Ouvrir le notebook dans Colab : [Lien √† venir]
2. T√©l√©verser vos documents `.md` dans Google Drive
3. Configurer votre cl√© API (voir section suivante)
4. Ex√©cuter les cellules dans l'ordre

Voir la section **Utilisation sur Google Colab** ci-dessous pour le guide d√©taill√©.

## Utilisation

### Option A : Utilisation sur Google Colab (recommand√©)

Le notebook `rag_demo.ipynb` est sp√©cialement con√ßu pour Google Colab avec int√©gration Google Drive.

#### 1. Pr√©paration de Google Drive

1. **Cr√©er un dossier pour vos documents** dans Google Drive :
   - Ouvrir [Google Drive](https://drive.google.com)
   - Cr√©er un nouveau dossier, par exemple : `m1_csmi_sgdb/data`
   - Y placer vos fichiers markdown (`.md`) √† indexer

2. **Organisation recommand√©e** :
   ```
   Mon Drive/
   ‚îî‚îÄ‚îÄ m1_csmi_sgdb/
       ‚îî‚îÄ‚îÄ rag_demo.ipynb
       ‚îî‚îÄ‚îÄ data/
           ‚îú‚îÄ‚îÄ introduction_rag.md
           ‚îú‚îÄ‚îÄ embeddings.md
           ‚îú‚îÄ‚îÄ chunking.md
           ‚îî‚îÄ‚îÄ ... (vos autres documents)
   ```

#### 2. Obtenir une cl√© API Codestral (Mistral AI)

Le notebook utilise par d√©faut l'API Codestral de Mistral AI (gratuite pour usage mod√©r√©).

1. **Cr√©er un compte Mistral AI** :
   - Aller sur [console.mistral.ai](https://console.mistral.ai)
   - Cr√©er un compte ou se connecter

2. **G√©n√©rer une cl√© API** :
   - Dans la console, aller dans **API Keys**
   - Cliquer sur **Create new key**
   - Copier la cl√© g√©n√©r√©e (format : `xxxxxxxxxxxxxxxxxxxxx`)
   - ‚ö†Ô∏è **Conserver cette cl√© en s√©curit√©** (elle ne sera affich√©e qu'une fois)

3. **V√©rifier les quotas** :
   - Plan gratuit : limites de requ√™tes/mois
   - Pour usage intensif : envisager un plan payant

#### 3. Configuration du notebook Colab

1. **Ouvrir le notebook** :
   - Option A : T√©l√©verser `rag_demo.ipynb` sur Google Colab
   - Option B : Ouvrir depuis GitHub (si disponible)

2. **Ex√©cuter la cellule 1** : Installation des d√©pendances
   ```python
   !pip install -q sentence-transformers torch numpy requests
   ```

3. **Ex√©cuter la cellule 2** : Montage de Google Drive
   - Autoriser l'acc√®s √† votre Drive quand demand√©
   - Modifier le chemin `DRIVE_DATA_DIR` pour pointer vers votre dossier :
     ```python
     DRIVE_DATA_DIR = '/content/drive/MyDrive/m1_csmi_sgdb/data'  # ‚Üê Modifier ici
     ```

4. **Ex√©cuter la cellule 4** : Configuration de la cl√© API

   **M√©thode recommand√©e - Secrets Colab** (s√©curis√©) :
   - Cliquer sur l'ic√¥ne üîë **Secrets** dans la barre lat√©rale gauche
   - Cliquer sur **Add new secret**
   - Nom : `CODESTRAL_API_KEY`
   - Valeur : votre cl√© copi√©e depuis Mistral AI
   - Activer **Notebook access**
   - Dans la cellule 4, d√©commenter et adapter :
     ```python
     from google.colab import userdata
     os.environ['CODESTRAL_API_KEY'] = userdata.get('CODESTRAL_API_KEY')
     ```

   **M√©thode alternative** (moins s√©curis√©, pour tests uniquement) :
   ```python
   os.environ['CODESTRAL_API_KEY'] = 'votre_cl√©_ici'  # ‚ö†Ô∏è Ne pas partager le notebook
   ```

5. **Ex√©cuter les cellules suivantes** dans l'ordre :
   - Cellule 5 : D√©finition des providers et classe RAG
   - Cellule 6 : Initialisation et chargement des documents
   - Cellules 7-10 : Exemples et tests

#### 4. Utilisation interactive

Une fois initialis√©, utiliser la fonction `ask()` :

```python
# Poser une question
ask("Qu'est-ce que le RAG ?")

# Avec plus de sources
ask("Comment fonctionnent les embeddings ?", top_k=10)
```

#### 5. Changer de provider LLM

Le notebook supporte plusieurs providers. Pour changer :

```python
# Dans la cellule 6, modifier :
PROVIDER = "MISTRAL_LARGE"  # ou "IRMA_LLMCODE", "PALGANIA_QWEN3", etc.

# Ou dynamiquement apr√®s initialisation :
rag.configure_provider("MISTRAL_LARGE")
```

**Providers disponibles** :
- `MISTRAL_CODESTRAL` (d√©faut) - Codestral via API Mistral
- `MISTRAL_LARGE` - Mistral Large (n√©cessite cl√© API)
- `IRMA_LLMCODE` - Serveur IRMA (pas de cl√© n√©cessaire)
- `PALGANIA_QWEN3` - Serveur Palgania
- `LOCAL_QWEN_CODER` - Serveur local (si vous avez un serveur LLM)

#### 6. Sauvegarder les r√©sultats

La cellule 12 montre comment sauvegarder les r√©sultats dans Google Drive :

```python
output_path = '/content/drive/MyDrive/rag_results.txt'
# Le fichier sera automatiquement synchronis√© avec votre Drive
```

### Option B : Script Python local (rag_demo.py)

Pour une utilisation en ligne de commande sans notebook.

Pour compiler les slides Typst en PDF :
```bash
typst compile doc/slides_rag.typ
```

### Option B : Script Python local (rag_demo.py)

Pour une utilisation en ligne de commande sans notebook.

#### Configuration des providers

Le syst√®me supporte plusieurs providers LLM via le fichier `llm_providers.py`.

**S√©lectionner un provider** :
```bash
# Via variable d'environnement
export LLM_PROVIDER=MISTRAL_CODESTRAL
export CODESTRAL_API_KEY=votre_cl√©_ici

# Puis lancer
python rag_demo.py
```

**Ou modifier directement dans le code** :
```python
# Dans rag_demo.py, ligne ~330
rag = SimpleRAG(data_dir="data_big", provider_name="MISTRAL_CODESTRAL")
```

#### Lancer la d√©monstration

```bash
python rag_demo.py
```

Le script va :
1. Charger les documents markdown du dossier configur√©
2. Cr√©er des embeddings avec le mod√®le `all-MiniLM-L6-v2`
3. Effectuer des recherches de similarit√© sur des exemples de questions
4. Proposer un mode interactif pour tester vos propres questions

#### Mode interactif

Apr√®s les exemples, le script propose un mode interactif o√π vous pouvez poser vos propres questions sur le RAG.

### Compiler les slides

Pour compiler les slides Typst en PDF :
```bash
typst compile doc/slides_rag.typ
```

## Contenu du cours

Les slides couvrent :
- Introduction et motivation
- Concepts fondamentaux du RAG
- Architecture d'un syst√®me RAG
- Embeddings et similarit√©
- Stockage vectoriel
- R√©cup√©ration de documents
- G√©n√©ration augment√©e
- Techniques avanc√©es (HyDE, Re-ranking, Multi-query)
- M√©triques d'√©valuation (Pr√©cision, Rappel, MRR, NDCG)
- Mise en pratique

## Technologies utilis√©es

- **Sentence Transformers** : Mod√®les d'embeddings (all-MiniLM-L6-v2)
- **PyTorch** : Backend pour les mod√®les d'embedding
- **NumPy** : Calculs de similarit√© cosinus
- **Requests** : Appels aux APIs LLM
- **Typst** : Syst√®me de composition des slides
- **Google Colab** : Environnement notebook cloud (optionnel)

## Configuration des providers LLM

Le syst√®me utilise une architecture modulaire avec support de multiples providers via `llm_providers.py`.

### Providers disponibles

| Provider | Description | Cl√© API requise | URL |
|----------|-------------|-----------------|-----|
| **MISTRAL_CODESTRAL** | Codestral (d√©faut) | `CODESTRAL_API_KEY` | `codestral.mistral.ai` |
| **MISTRAL_LARGE** | Mistral Large | `MISTRAL_API_KEY` | `api.mistral.ai` |
| **IRMA_LLMCODE** | Serveur IRMA | Non | `llmcode.math.unistra.fr:8090` |
| **PALGANIA_QWEN3** | Qwen3-30B | `TEXTSYNTH_API_KEY` | `palgania.ovh:8106` |
| **LOCAL_QWEN_CODER** | Serveur local | Non | `127.0.0.1:8080` |

### Configuration des cl√©s API

**M√©thode 1 - Variables d'environnement (recommand√©)** :
```bash
export CODESTRAL_API_KEY="votre_cl√©_ici"
export MISTRAL_API_KEY="votre_cl√©_ici"
```

**M√©thode 2 - Dans le code** :
```python
rag = SimpleRAG(
    data_dir="data",
    provider_name="MISTRAL_CODESTRAL",
    api_key="votre_cl√©_directement"
)
```

**M√©thode 3 - Secrets Colab** (notebook uniquement) :
```python
from google.colab import userdata
os.environ['CODESTRAL_API_KEY'] = userdata.get('CODESTRAL_API_KEY')
```

### Changer de provider

**Au d√©marrage** :
```python
rag = SimpleRAG(provider_name="MISTRAL_LARGE")
```

**Dynamiquement** :
```python
rag.configure_provider("IRMA_LLMCODE")
rag.configure_provider("PALGANIA_QWEN3", override_model="Qwen3-72B")
```

## Troubleshooting

### Erreur : "Cl√© API manquante"
- V√©rifier que la variable d'environnement est bien d√©finie
- Dans Colab : v√©rifier que le secret est activ√© et accessible au notebook

### Erreur : "Timeout" ou "Connection refused"
- V√©rifier que le serveur LLM est accessible
- Pour serveurs locaux : v√©rifier qu'il tourne sur le bon port
- Pour APIs cloud : v√©rifier votre connexion internet

### Documents non charg√©s depuis Drive
- V√©rifier le chemin `DRIVE_DATA_DIR` (doit pointer vers le bon dossier)
- S'assurer que Drive est bien mont√© (cellule 2)
- V√©rifier que les fichiers ont l'extension `.md`

### Mod√®le d'embedding trop lent
- Premi√®re ex√©cution : t√©l√©chargement du mod√®le (~100 MB)
- Utiliser un GPU dans Colab : Runtime ‚Üí Change runtime type ‚Üí GPU

## Ressources suppl√©mentaires

- [Documentation Mistral AI](https://docs.mistral.ai/)
- [Console Mistral AI](https://console.mistral.ai) - Cr√©er cl√©s API
- [Sentence Transformers](https://www.sbert.net/)
- [Guide RAG de LangChain](https://python.langchain.com/docs/use_cases/question_answering/)
- [Google Colab Secrets](https://medium.com/@parthdasawant/how-to-use-secrets-in-google-colab-450c38e3ec75)

## Notes

- La premi√®re ex√©cution t√©l√©chargera le mod√®le d'embedding (~100 MB)
- Les mod√®les sont mis en cache automatiquement par HuggingFace
- L'API REST doit √™tre accessible pour la g√©n√©ration de r√©ponses
- Les embeddings sont calcul√©s localement (pas via l'API)

## Licence

Voir le fichier LICENSE
