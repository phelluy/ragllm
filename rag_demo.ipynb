{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c2e3f8",
   "metadata": {
    "id": "12c2e3f8"
   },
   "source": [
    "## 1. Installation des d√©pendances\n",
    "\n",
    "Installation des biblioth√®ques n√©cessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "319b2f3c",
   "metadata": {
    "executionInfo": {
     "elapsed": 4438,
     "status": "ok",
     "timestamp": 1764189425457,
     "user": {
      "displayName": "Philippe Helluy",
      "userId": "07790988964707423334"
     },
     "user_tz": -60
    },
    "id": "319b2f3c"
   },
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers torch numpy requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7434794",
   "metadata": {
    "id": "a7434794"
   },
   "source": [
    "## 2. Connexion √† Google Drive\n",
    "\n",
    "Montage de Google Drive pour acc√©der aux documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0f464f9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31581,
     "status": "ok",
     "timestamp": 1764189462940,
     "user": {
      "displayName": "Philippe Helluy",
      "userId": "07790988964707423334"
     },
     "user_tz": -60
    },
    "id": "c0f464f9",
    "outputId": "76df463f-da9d-49cb-b9ec-8dbafa43315c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Google Drive mont√©. R√©pertoire de donn√©es: /content/drive/MyDrive/m1_csmi_sgdb/data\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# D√©finir le chemin vers vos documents dans Google Drive\n",
    "# Exemple: DRIVE_DATA_DIR = '/content/drive/MyDrive/rag_documents'\n",
    "DRIVE_DATA_DIR = '/content/drive/MyDrive/m1_csmi_sgdb/data'\n",
    "\n",
    "print(f\"Google Drive mont√©. R√©pertoire de donn√©es: {DRIVE_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f86cf3",
   "metadata": {
    "id": "78f86cf3"
   },
   "source": [
    "## 3. Configuration des providers LLM\n",
    "\n",
    "D√©finition des diff√©rents providers compatibles OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61549c1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1764189479647,
     "user": {
      "displayName": "Philippe Helluy",
      "userId": "07790988964707423334"
     },
     "user_tz": -60
    },
    "id": "e61549c1",
    "outputId": "8631a5aa-026a-48d1-981a-a4215e849a83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providers disponibles: ['MISTRAL_LARGE', 'MISTRAL_CODESTRAL', 'LOCAL_QWEN_CODER', 'IRMA_LLMCODE', 'PALGANIA_QWEN3']\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Dict\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class ProviderConfig:\n",
    "    name: str\n",
    "    url: str\n",
    "    model: str\n",
    "    api_key_env: Optional[str] = None\n",
    "\n",
    "    def api_key(self) -> str:\n",
    "        if not self.api_key_env:\n",
    "            return \"\"\n",
    "        return os.getenv(self.api_key_env, \"\")\n",
    "\n",
    "# Registre des providers connus\n",
    "PROVIDERS: Dict[str, ProviderConfig] = {\n",
    "    \"MISTRAL_LARGE\": ProviderConfig(\n",
    "        name=\"MISTRAL_LARGE\",\n",
    "        url=\"https://api.mistral.ai/v1/chat/completions\",\n",
    "        model=\"open-mistral-nemo\",\n",
    "        api_key_env=\"MISTRAL_API_KEY\",\n",
    "    ),\n",
    "    \"MISTRAL_CODESTRAL\": ProviderConfig(\n",
    "        name=\"MISTRAL_CODESTRAL\",\n",
    "        url=\"https://codestral.mistral.ai/v1/chat/completions\",\n",
    "        model=\"codestral-latest\",\n",
    "        api_key_env=\"CODESTRAL_API_KEY\",\n",
    "    ),\n",
    "    \"LOCAL_QWEN_CODER\": ProviderConfig(\n",
    "        name=\"LOCAL_QWEN_CODER\",\n",
    "        url=\"http://127.0.0.1:8080/v1/chat/completions\",\n",
    "        model=\"qwen3-32b-instruct\",\n",
    "        api_key_env=None,\n",
    "    ),\n",
    "    \"IRMA_LLMCODE\": ProviderConfig(\n",
    "        name=\"IRMA_LLMCODE\",\n",
    "        url=\"http://llmcode.math.unistra.fr:8090/v1/chat/completions\",\n",
    "        model=\"qwen2.5-coder-instruct\",\n",
    "        api_key_env=None,\n",
    "    ),\n",
    "    \"PALGANIA_QWEN3\": ProviderConfig(\n",
    "        name=\"PALGANIA_QWEN3\",\n",
    "        url=\"https://palgania.ovh:8106/v1/chat/completions\",\n",
    "        model=\"Qwen3-30B\",\n",
    "        api_key_env=\"TEXTSYNTH_API_KEY\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "#DEFAULT_PROVIDER = \"MISTRAL_CODESTRAL\"\n",
    "DEFAULT_PROVIDER = \"MISTRAL_LARGE\"\n",
    "\n",
    "\n",
    "def get_provider(name: Optional[str] = None,\n",
    "                 override_model: Optional[str] = None,\n",
    "                 override_url: Optional[str] = None,\n",
    "                 api_key: Optional[str] = None) -> ProviderConfig:\n",
    "    \"\"\"Retourne la configuration d'un provider.\"\"\"\n",
    "    if name is None:\n",
    "        name = DEFAULT_PROVIDER\n",
    "    if name not in PROVIDERS:\n",
    "        raise ValueError(f\"Provider inconnu: {name}. Providers disponibles: {list(PROVIDERS.keys())}\")\n",
    "\n",
    "    base = PROVIDERS[name]\n",
    "    model = override_model or base.model\n",
    "    url = override_url or base.url\n",
    "    key = api_key if api_key is not None else base.api_key()\n",
    "\n",
    "    return ProviderConfig(name=name, url=url, model=model, api_key_env=base.api_key_env)\n",
    "\n",
    "print(f\"Providers disponibles: {list(PROVIDERS.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37347bcc",
   "metadata": {
    "id": "37347bcc"
   },
   "source": [
    "## 4. Configuration des cl√©s API\n",
    "\n",
    "D√©finissez vos cl√©s API ici (pour Colab, utilisez les secrets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a75665d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1108,
     "status": "ok",
     "timestamp": 1764189498349,
     "user": {
      "displayName": "Philippe Helluy",
      "userId": "07790988964707423334"
     },
     "user_tz": -60
    },
    "id": "a75665d5",
    "outputId": "b9821b4a-16fd-4072-b0f2-c7525b9580a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cl√©s API configur√©es (si n√©cessaire)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Pour Google Colab, vous pouvez utiliser les secrets ou d√©finir directement:\n",
    "from google.colab import userdata\n",
    "os.environ['CODESTRAL_API_KEY'] = userdata.get('CODESTRAL_API_KEY')\n",
    "os.environ['MISTRAL_API_KEY'] = userdata.get('MISTRAL_API_KEY')\n",
    "\n",
    "# Ou d√©finir directement (‚ö†Ô∏è ne commitez jamais vos cl√©s!):\n",
    "# os.environ['MISTRAL_API_KEY'] = 'votre_cl√©_ici'\n",
    "#os.environ['CODESTRAL_API_KEY'] = 'votre cl√©'\n",
    "# os.environ['TEXTSYNTH_API_KEY'] = 'votre_cl√©_ici'\n",
    "\n",
    "print(\"Cl√©s API configur√©es (si n√©cessaire)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c5ac62",
   "metadata": {
    "id": "03c5ac62"
   },
   "source": [
    "## 5. Classe SimpleRAG\n",
    "\n",
    "Impl√©mentation compl√®te du syst√®me RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3efa90d8",
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1764190442924,
     "user": {
      "displayName": "Philippe Helluy",
      "userId": "07790988964707423334"
     },
     "user_tz": -60
    },
    "id": "3efa90d8"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Optional\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import urllib3\n",
    "\n",
    "# D√©sactiver les avertissements SSL\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "\n",
    "class SimpleRAG:\n",
    "    \"\"\"Syst√®me RAG minimaliste avec s√©lection modulaire de provider LLM.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = \"data\",\n",
    "        provider_name: Optional[str] = None,\n",
    "        override_model: Optional[str] = None,\n",
    "        override_url: Optional[str] = None,\n",
    "        api_key: Optional[str] = None,\n",
    "    ):\n",
    "        \"\"\"Initialise le syst√®me RAG.\"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "\n",
    "        # Mod√®le d'embedding\n",
    "        print(\"Chargement du mod√®le d'embedding...\")\n",
    "        embed_model_name = \"paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "        self.embedding_model = SentenceTransformer(embed_model_name)\n",
    "        print(f\"  ‚Üí Mod√®le {embed_model_name} charg√©\")\n",
    "\n",
    "        # Configuration du provider LLM\n",
    "        self._provider_cfg = get_provider(\n",
    "            name=provider_name,\n",
    "            override_model=override_model,\n",
    "            override_url=override_url,\n",
    "            api_key=api_key,\n",
    "        )\n",
    "        self.api_url = self._provider_cfg.url\n",
    "        self.model = self._provider_cfg.model\n",
    "        self.api_key = self._provider_cfg.api_key_env and os.getenv(self._provider_cfg.api_key_env, \"\") or \"\"\n",
    "\n",
    "        print(f\"Provider: {self._provider_cfg.name} | URL: {self.api_url} | Mod√®le: {self.model}\")\n",
    "        if self._provider_cfg.api_key_env:\n",
    "            if not self.api_key:\n",
    "                print(f\"‚ö†Ô∏è Cl√© API manquante. D√©finissez '{self._provider_cfg.api_key_env}'\")\n",
    "            else:\n",
    "                print(f\"‚úì Cl√© API charg√©e\")\n",
    "\n",
    "    def load_documents(self) -> None:\n",
    "        \"\"\"Charge tous les fichiers markdown du r√©pertoire.\"\"\"\n",
    "        print(f\"\\nChargement des documents depuis {self.data_dir}/...\")\n",
    "\n",
    "        md_files = glob.glob(os.path.join(self.data_dir, \"*.md\"))\n",
    "\n",
    "        for filepath in md_files:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n",
    "\n",
    "                for para in paragraphs:\n",
    "                    self.documents.append({\n",
    "                        'text': para,\n",
    "                        'source': os.path.basename(filepath)\n",
    "                    })\n",
    "\n",
    "        print(f\"  ‚Üí {len(self.documents)} chunks charg√©s depuis {len(md_files)} fichiers\")\n",
    "\n",
    "    def create_embeddings(self) -> None:\n",
    "        \"\"\"G√©n√®re les embeddings pour tous les documents.\"\"\"\n",
    "        print(\"\\nCr√©ation des embeddings...\")\n",
    "\n",
    "        texts = [doc['text'] for doc in self.documents]\n",
    "        self.embeddings = self.embedding_model.encode(\n",
    "            texts,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True\n",
    "        )\n",
    "\n",
    "        print(f\"  ‚Üí {len(self.embeddings)} embeddings cr√©√©s (dimension: {self.embeddings.shape[1]})\")\n",
    "\n",
    "    def search(self, query: str, top_k: int = 3) -> List[Tuple[dict, float]]:\n",
    "        \"\"\"Recherche les documents les plus similaires √† la requ√™te.\"\"\"\n",
    "        query_embedding = self.embedding_model.encode(query, convert_to_numpy=True)\n",
    "\n",
    "        similarities = np.dot(self.embeddings, query_embedding) / (\n",
    "            np.linalg.norm(self.embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
    "        )\n",
    "\n",
    "        top_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "\n",
    "        results = [\n",
    "            (self.documents[idx], float(similarities[idx]))\n",
    "            for idx in top_indices\n",
    "        ]\n",
    "\n",
    "        return results\n",
    "\n",
    "    def test_embedding(self):\n",
    "      sentence1 = \"le chat dort\"\n",
    "      sentence2 = \"le matou fait la sieste\"\n",
    "      embed1= self.embedding_model.encode(sentence1, convert_to_numpy=True)\n",
    "      embed2= self.embedding_model.encode(sentence2, convert_to_numpy=True)\n",
    "      print(f\"Similarity ('{sentence1}', '{sentence2}'): {np.dot(embed1, embed2) / (np.linalg.norm(embed1) * np.linalg.norm(embed2)):.4f}\")\n",
    "\n",
    "      sentence3 = \"le chat ne dort pas\"\n",
    "      embed3= self.embedding_model.encode(sentence3, convert_to_numpy=True)\n",
    "      print(f\"Similarity ('{sentence1}', '{sentence3}'): {np.dot(embed1, embed3) / (np.linalg.norm(embed1) * np.linalg.norm(embed3)):.4f}\")\n",
    "\n",
    "      sentence4 = \"la voiture est en panne\"\n",
    "      embed4= self.embedding_model.encode(sentence4, convert_to_numpy=True)\n",
    "      print(f\"Similarity ('{sentence1}', '{sentence4}'): {np.dot(embed1, embed4) / (np.linalg.norm(embed1) * np.linalg.norm(embed4)):.4f}\")\n",
    "\n",
    "      sentence5 = \"the cat is sleeping\"\n",
    "      embed5= self.embedding_model.encode(sentence5, convert_to_numpy=True)\n",
    "      print(f\"Similarity ('{sentence1}', '{sentence5}'): {np.dot(embed1, embed5) / (np.linalg.norm(embed1) * np.linalg.norm(embed5)):.4f}\")\n",
    "\n",
    "    def configure_provider(\n",
    "        self,\n",
    "        provider_name: str,\n",
    "        override_model: Optional[str] = None,\n",
    "        override_url: Optional[str] = None,\n",
    "        api_key: Optional[str] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Change dynamiquement la configuration du provider LLM.\"\"\"\n",
    "        cfg = get_provider(\n",
    "            name=provider_name,\n",
    "            override_model=override_model,\n",
    "            override_url=override_url,\n",
    "            api_key=api_key,\n",
    "        )\n",
    "        self._provider_cfg = cfg\n",
    "        self.api_url = cfg.url\n",
    "        self.model = override_model or cfg.model\n",
    "        self.api_key = api_key if api_key is not None else (cfg.api_key_env and os.getenv(cfg.api_key_env, \"\") or \"\")\n",
    "        print(f\"Provider reconfigur√©: {provider_name} | URL: {self.api_url} | Mod√®le: {self.model}\")\n",
    "\n",
    "    def generate_with_llm(self, query: str, context_docs: List[dict]) -> str:\n",
    "        \"\"\"G√©n√®re une r√©ponse en utilisant l'API REST.\"\"\"\n",
    "        context = \"\\n\\n\".join([doc['text'] for doc in context_docs])\n",
    "\n",
    "        prompt = f\"\"\"Bas√© sur le contexte suivant (et uniquement sur ce contexte), r√©ponds √† la question de mani√®re concise et pr√©cise.\n",
    "\n",
    "Contexte:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "R√©ponse:\"\"\"\n",
    "\n",
    "        #print(prompt)\n",
    "\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt,\n",
    "                }\n",
    "            ],\n",
    "            \"temperature\": 0.3,\n",
    "            \"max_tokens\": 2000,\n",
    "            \"top_p\": 0.9,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            headers = {\"Content-Type\": \"application/json\"}\n",
    "            if self.api_key:\n",
    "                headers[\"Authorization\"] = f\"Bearer {self.api_key}\"\n",
    "\n",
    "            verify_ssl = self.api_url.startswith(\"https://\")\n",
    "            response = requests.post(\n",
    "                self.api_url,\n",
    "                json=payload,\n",
    "                headers=headers,\n",
    "                verify=verify_ssl,\n",
    "                timeout=60,\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "\n",
    "            result = response.json()\n",
    "\n",
    "            if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "                return result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            else:\n",
    "                return \"Erreur : format de r√©ponse inattendu de l'API\"\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return f\"Erreur lors de l'appel √† l'API : {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113cdf39",
   "metadata": {
    "id": "113cdf39"
   },
   "source": [
    "## 6. Initialisation du syst√®me RAG\n",
    "\n",
    "Cr√©er une instance et charger les documents depuis Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a66fa85",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223,
     "referenced_widgets": [
      "d18209f8cd9048e488cfa57f4643d9ae",
      "415c03f153d74ea0aba0f9faea2c5bea",
      "b5c208d1d8834a8ab90aa2e93425cd20",
      "556c450cfe5e4eb58b372dd74573a75d",
      "f6a5547b902f4eb0a4055acf3887dfef",
      "fa2733e4e7c74f64870d9693b341feab",
      "42e7952ea1544dc5922439e54f219888",
      "576109ea617a4291a6f55a756b4fd302",
      "63522aba08e94f7f96c972ac093c9951",
      "89e8fd28f2b04b19a09ec2c30695f13f",
      "da0ef5cb9d244871a95f203aa21f26be"
     ]
    },
    "executionInfo": {
     "elapsed": 7879,
     "status": "ok",
     "timestamp": 1764190456489,
     "user": {
      "displayName": "Philippe Helluy",
      "userId": "07790988964707423334"
     },
     "user_tz": -60
    },
    "id": "3a66fa85",
    "outputId": "ed55d185-f87b-4358-eb46-9372f5d25de5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du mod√®le d'embedding...\n",
      "  ‚Üí Mod√®le paraphrase-multilingual-MiniLM-L12-v2 charg√©\n",
      "Provider: MISTRAL_CODESTRAL | URL: https://codestral.mistral.ai/v1/chat/completions | Mod√®le: codestral-latest\n",
      "‚úì Cl√© API charg√©e\n",
      "\n",
      "Chargement des documents depuis /content/drive/MyDrive/m1_csmi_sgdb/data/...\n",
      "  ‚Üí 1784 chunks charg√©s depuis 1 fichiers\n",
      "\n",
      "Cr√©ation des embeddings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d18209f8cd9048e488cfa57f4643d9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚Üí 1784 embeddings cr√©√©s (dimension: 384)\n"
     ]
    }
   ],
   "source": [
    "# Choisir le provider (None = DEFAULT_PROVIDER)\n",
    "# Options: \"MISTRAL_LARGE\", \"MISTRAL_CODESTRAL\", \"IRMA_LLMCODE\", \"PALGANIA_QWEN3\"\n",
    "PROVIDER = \"MISTRAL_CODESTRAL\"  # Changez selon vos besoins\n",
    "\n",
    "# Initialisation\n",
    "rag = SimpleRAG(data_dir=DRIVE_DATA_DIR, provider_name=PROVIDER)\n",
    "\n",
    "# Chargement et indexation\n",
    "rag.load_documents()\n",
    "rag.create_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "W8ghnZ_jos5M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1764190461448,
     "user": {
      "displayName": "Philippe Helluy",
      "userId": "07790988964707423334"
     },
     "user_tz": -60
    },
    "id": "W8ghnZ_jos5M",
    "outputId": "9c51b4d8-431e-4d7a-e003-6ef0ee606282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity ('le chat dort', 'le matou fait la sieste'): 0.1057\n",
      "Similarity ('le chat dort', 'le chat ne dort pas'): 0.8798\n",
      "Similarity ('le chat dort', 'la voiture est en panne'): 0.1292\n",
      "Similarity ('le chat dort', 'the cat is sleeping'): 0.9615\n"
     ]
    }
   ],
   "source": [
    "rag.test_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fafafd4",
   "metadata": {
    "id": "5fafafd4"
   },
   "source": [
    "## 7. Tester une recherche simple\n",
    "\n",
    "Rechercher des documents pertinents pour une question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fe52812",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1764190463886,
     "user": {
      "displayName": "Philippe Helluy",
      "userId": "07790988964707423334"
     },
     "user_tz": -60
    },
    "id": "8fe52812",
    "outputId": "bc1d569b-94b0-4009-99d4-bb1aba0c5a99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Quel est le titre de ce roman ?\n",
      "\n",
      "======================================================================\n",
      "\n",
      "[1] Score: 0.6751 | Source: chartreuse_de_parme_stendhal.md\n",
      "    Titre du roman: La Chartreuse de Parme...\n",
      "\n",
      "[2] Score: 0.4985 | Source: chartreuse_de_parme_stendhal.md\n",
      "    Publication: 1839\n",
      "Cat√©gorie(s): Fiction, Roman...\n",
      "\n",
      "[3] Score: 0.4709 | Source: chartreuse_de_parme_stendhal.md\n",
      "    Cette lettre contenait encore cinq ou six pages de d√©tails, elle √©tait √©crite en caract√®res microscopiques sur du papier tr√®s fin.\n",
      "\"Tout cela est fort beau et fort bien invent√©, se dit Fabrice; je doi...\n"
     ]
    }
   ],
   "source": [
    "query = \"Quel est le titre de ce roman ?\"\n",
    "\n",
    "print(f\"Question: {query}\\n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = rag.search(query, top_k=3)\n",
    "\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"\\n[{i}] Score: {score:.4f} | Source: {doc['source']}\")\n",
    "    print(f\"    {doc['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72503b35",
   "metadata": {
    "id": "72503b35"
   },
   "source": [
    "## 8. G√©n√©rer une r√©ponse avec le LLM\n",
    "\n",
    "Utiliser les documents r√©cup√©r√©s pour g√©n√©rer une r√©ponse via l'API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d854d7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1764190472316,
     "user": {
      "displayName": "Philippe Helluy",
      "userId": "07790988964707423334"
     },
     "user_tz": -60
    },
    "id": "7d854d7d",
    "outputId": "97299683-f231-491c-e27e-39cc62ce9462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Quel est le titre du roman ?\n",
      "\n",
      "G√©n√©ration de la r√©ponse...\n",
      "\n",
      "======================================================================\n",
      "R√âPONSE:\n",
      "======================================================================\n",
      "*La Chartreuse de Parme*.\n"
     ]
    }
   ],
   "source": [
    "query = \"Quel est le titre du roman ?\"\n",
    "\n",
    "print(f\"Question: {query}\\n\")\n",
    "\n",
    "# Recherche\n",
    "results = rag.search(query, top_k=5)\n",
    "context_docs = [doc for doc, score in results]\n",
    "\n",
    "# G√©n√©ration\n",
    "print(\"G√©n√©ration de la r√©ponse...\\n\")\n",
    "response = rag.generate_with_llm(query, context_docs)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"R√âPONSE:\")\n",
    "print(\"=\"*70)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0648a09a",
   "metadata": {
    "id": "0648a09a"
   },
   "source": [
    "## 9. Fonction helper pour Q&A interactive\n",
    "\n",
    "Cr√©er une fonction simple pour poser des questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "55b88ae2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1764190483377,
     "user": {
      "displayName": "Philippe Helluy",
      "userId": "07790988964707423334"
     },
     "user_tz": -60
    },
    "id": "55b88ae2",
    "outputId": "6b081941-0420-45bb-bfe5-cecdf3b75abb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonction ask() d√©finie. Utilisez: ask('votre question')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def ask(question: str, top_k: int = 10, show_sources: bool = True):\n",
    "    \"\"\"Pose une question au syst√®me RAG et affiche la r√©ponse.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"QUESTION: {question}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    # Recherche\n",
    "    results = rag.search(question, top_k=top_k)\n",
    "\n",
    "    if show_sources:\n",
    "        print(\"üìö Sources trouv√©es:\")\n",
    "        for i, (doc, score) in enumerate(results[:3], 1):\n",
    "            print(f\"  [{i}] {doc['source']} (score: {score:.3f})\")\n",
    "        print(\"etc.\")\n",
    "\n",
    "    # G√©n√©ration\n",
    "    context_docs = [doc for doc, score in results]\n",
    "\n",
    "    # √âcrire la question et le contexte dans un fichier\n",
    "    output_context_path = os.path.join(DRIVE_DATA_DIR, 'contexte.txt')\n",
    "    with open(output_context_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(f\"Question: {question}\\n\\n\")\n",
    "        f.write(\"Contexte utilis√©:\\n\")\n",
    "        for doc in context_docs:\n",
    "            f.write(f\"---\\nSource: {doc['source']}\\n{doc['text']}\\n\")\n",
    "        print(f\"Question et contexte sauvegard√©s dans : {output_context_path}\")\n",
    "\n",
    "    response = rag.generate_with_llm(question, context_docs)\n",
    "\n",
    "    print(\"üí¨ R√âPONSE:\")\n",
    "    print(\"-\"*70)\n",
    "    print(response)\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    return response\n",
    "\n",
    "print(\"Fonction ask() d√©finie. Utilisez: ask('votre question')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13270c6d",
   "metadata": {
    "id": "13270c6d"
   },
   "source": [
    "## 10. Exemples d'utilisation\n",
    "\n",
    "Testez le syst√®me avec diff√©rentes questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "771e4cd0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "executionInfo": {
     "elapsed": 851,
     "status": "ok",
     "timestamp": 1764189884318,
     "user": {
      "displayName": "Philippe Helluy",
      "userId": "07790988964707423334"
     },
     "user_tz": -60
    },
    "id": "771e4cd0",
    "outputId": "33dc8ab8-a837-4e05-b50a-e72b1fb8ba58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "QUESTION: Quel est le titre du roman ?\n",
      "======================================================================\n",
      "\n",
      "üìö Sources trouv√©es:\n",
      "  [1] chartreuse_de_parme_stendhal.md (score: 0.663)\n",
      "  [2] chartreuse_de_parme_stendhal.md (score: 0.491)\n",
      "  [3] chartreuse_de_parme_stendhal.md (score: 0.436)\n",
      "etc.\n",
      "Question et contexte sauvegard√©s dans : /content/drive/MyDrive/m1_csmi_sgdb/data/contexte.txt\n",
      "Bas√© sur le contexte suivant (et uniquement sur ce contexte), r√©ponds √† la question de mani√®re concise et pr√©cise.\n",
      "\n",
      "Contexte:\n",
      "Titre du roman: La Chartreuse de Parme\n",
      "\n",
      "Publication: 1839\n",
      "Cat√©gorie(s): Fiction, Roman\n",
      "\n",
      "- Quelle bassesse √† moi! s'√©tait-elle √©cri√©e: dire du mal √† Fabrice de la femme qu'il aime!...\n",
      "\n",
      "- Rendez-moi ce papier, dit la duchesse, et, devant lui, elle le br√ªla √† la bougie.\n",
      "\"Il ne faut pas, ajouta-t-elle, que mon nom paraisse si vous √™tes pris et ex√©cut√©, car il y va de votre t√™te.\n",
      "\n",
      "Cette lettre contenait encore cinq ou six pages de d√©tails, elle √©tait √©crite en caract√®res microscopiques sur du papier tr√®s fin.\n",
      "\"Tout cela est fort beau et fort bien invent√©, se dit Fabrice; je dois une reconnaissance √©ternelle au comte et √† la duchesse; ils croiront peut-√™tre que j'ai eu peur, mais je ne me sauverai point. Est-ce que jamais l'on se sauva d'un lieu o√π l'on est au comble du bonheur, pour aller se jeter dans un exil affreux o√π tout manquera, jusqu'√† l'air pour respirer? Que ferais-je au bout\n",
      "d'un mois que je serais √† Florence? je prendrais un d√©guisement pour venir r√¥der aupr√®s de la porte de cette forteresse, et t√¢cher d'√©pier un regard!\"\n",
      "\n",
      "C'√©tait la copie du billet du prince que la duchesse voulait envoyer √† Fabrice; elle ne put r√©sister au plaisir de l'amuser, et ajouta un mot sur la sc√®ne qui avait amen√© le billet; ce mot devint une lettre de dix pages. Elle fit rappeler le postillon.\n",
      "\n",
      "Elle raconta la petite aventure dont le lecteur se souvient peut-√™tre.\n",
      "\n",
      "- Je supplie Votre Altesse de lire toute la fable.\n",
      "\n",
      "Fabrice allait partir au premier moment d'humeur, lorsqu'il apprit que la Fausta devait chanter chez la duchesse Sanseverina.\"Peut-√™tre que cette voix sublime ach√®vera d'enflammer mon coeur\", se dit-il; et il osa bien s'introduire d√©guis√© dans ce palais o√π tous les yeux le connaissaient. Qu'on juge de l'√©motion de la duchesse, lorsque tout √† fait vers la fin du concert elle remarqua un homme en livr√©e de chasseur, debout pr√®s de la porte du grand salon; cette tournure rappelait quelqu'un. Elle chercha le comte Mosca qui seulement alors lui apprit l'insigne et vraiment incroyable folie de Fabrice. Il la prenait tr√®s bien. Cet amour pour une autre que la duchesse lui plaisait fort; le comte, parfaitement galant homme, hors de la politique, agissait d'apr√®s cette maxime qu'il ne pouvait trouver le bonheur qu'autant que la duchesse serait heureuse. - Je le sauverai de lui-m√™me, dit-il √† son amie; jugez de la joie de nos ennemis si on l'arr√™tait dans ce palais! Aussi ai-je ici plus de cent hommes √† moi, et c'est pour cela que je vous ai fait demander les clefs du grand ch√¢teau d'eau. Il se porte pour amoureux fou de\n",
      "la Fausta? et jusqu'ici ne peut l'enlever au comte $\\mathrm{M}^{* * *}$ qui donne √† cette folle une existence de reine.\n",
      "\n",
      "La lettre √©tait bien plaisante; le comte employait les termes les plus lugubres, et cependant la joie la plus vive √©clatait √† chaque mot; il √©vitait les d√©tails sur le genre de mort du prince, et finissait sa lettre par ces mots:\n",
      "\n",
      "Question: Quel est le titre du roman ?\n",
      "\n",
      "R√©ponse:\n",
      "üí¨ R√âPONSE:\n",
      "----------------------------------------------------------------------\n",
      "*La Chartreuse de Parme*.\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'*La Chartreuse de Parme*.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple 1\n",
    "ask(\"Quel est le titre du roman ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0799abb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "executionInfo": {
     "elapsed": 1262,
     "status": "ok",
     "timestamp": 1764190702856,
     "user": {
      "displayName": "Philippe Helluy",
      "userId": "07790988964707423334"
     },
     "user_tz": -60
    },
    "id": "b0799abb",
    "outputId": "ba5fc87f-8810-47cb-b781-61bf7ffd687b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "QUESTION: Qui est Cl√©lia ?\n",
      "======================================================================\n",
      "\n",
      "üìö Sources trouv√©es:\n",
      "  [1] chartreuse_de_parme_stendhal.md (score: 0.611)\n",
      "  [2] chartreuse_de_parme_stendhal.md (score: 0.605)\n",
      "  [3] chartreuse_de_parme_stendhal.md (score: 0.594)\n",
      "etc.\n",
      "Question et contexte sauvegard√©s dans : /content/drive/MyDrive/m1_csmi_sgdb/data/contexte.txt\n",
      "üí¨ R√âPONSE:\n",
      "----------------------------------------------------------------------\n",
      "Cl√©lia est une jeune femme noble et pieuse, amoureuse de Fabrice, mais contrainte par un voeu √† la Madone de ne jamais le revoir. Elle sacrifie sa vie pour lui, malgr√© son amour intense, et maintient une correspondance secr√®te avec lui en prison.\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Cl√©lia est une jeune femme noble et pieuse, amoureuse de Fabrice, mais contrainte par un voeu √† la Madone de ne jamais le revoir. Elle sacrifie sa vie pour lui, malgr√© son amour intense, et maintient une correspondance secr√®te avec lui en prison.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple 2\n",
    "ask(\"Qui est Cl√©lia ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "471280f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1764190720813,
     "user": {
      "displayName": "Philippe Helluy",
      "userId": "07790988964707423334"
     },
     "user_tz": -60
    },
    "id": "471280f5",
    "outputId": "83d7a088-2ea3-45a1-be5c-e831b8574efc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenu du r√©pertoire /content/drive/MyDrive/m1_csmi_sgdb/data:\n",
      "['chartreuse_de_parme_stendhal.md', 'contexte.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# V√©rifier le contenu du r√©pertoire de donn√©es de Google Drive\n",
    "print(f\"Contenu du r√©pertoire {DRIVE_DATA_DIR}:\")\n",
    "if os.path.exists(DRIVE_DATA_DIR):\n",
    "    print(os.listdir(DRIVE_DATA_DIR))\n",
    "else:\n",
    "    print(f\"Le r√©pertoire {DRIVE_DATA_DIR} n'existe pas ou n'est pas accessible.\")\n",
    "\n",
    "# Si le probl√®me persiste, vous pouvez tenter de re-monter Google Drive:\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)\n",
    "# print(f\"Google Drive re-mont√©. R√©pertoire de donn√©es: {DRIVE_DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca98b753",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "executionInfo": {
     "elapsed": 529,
     "status": "ok",
     "timestamp": 1763647013993,
     "user": {
      "displayName": "Philippe Helluy",
      "userId": "07790988964707423334"
     },
     "user_tz": -60
    },
    "id": "ca98b753",
    "outputId": "a7262630-b8ed-4bbd-be88-a8e591fd1d4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "QUESTION: Quel est le titre du roman ?\n",
      "======================================================================\n",
      "\n",
      "üìö Sources trouv√©es:\n",
      "  [1] chartreuse_de_parme_stendhal.md (score: 0.603)\n",
      "  [2] chartreuse_de_parme_stendhal.md (score: 0.582)\n",
      "  [3] chartreuse_de_parme_stendhal.md (score: 0.581)\n",
      "etc.\n",
      "Question et contexte sauvegard√©s dans : /content/drive/MyDrive/m1_csmi_sgdb/data/contexte.txt\n",
      "Bas√© sur le contexte suivant (et uniquement sur ce contexte), r√©ponds √† la question de mani√®re concise et pr√©cise.\n",
      "\n",
      "Contexte:\n",
      "- Quel homme est-ce que Dugnani, vicaire de Saint-Paul?\n",
      "- Un petit esprit et une grande ambition r√©pondit l'archev√™que, peu de scrupules et une extr√™me pauvret√©, car nous en avons des vices!\n",
      "- Tudieu, monseigneur! s'√©cria le ministre, vous peignez comme Tacite.\n",
      "\n",
      "Et il part au galop; son camarade le suit. Tout cela fut fait en un clin d'oeil.\n",
      "\n",
      "Il fallait que dans son exil √† Romagnano Fabrice:\n",
      "1 Ne manqu√¢t pas d'aller √† la messe tous les jours, pr√Æt pour confesseur un homme d'esprit, d√©vou√© √† la cause de la monarchie, et ne lui avou√¢t, au tribunal de la p√©nitence, que des sentiments fort irr√©prochables.\n",
      "\n",
      "Il y avait √† peine une demi-heure que Fabrice √©tait en sentinelle au pont, quand il vit arriver six chasseurs mont√©s et trois √† pied; il leur communique\n",
      "l'ordre du colonel.\n",
      "\n",
      "Fabrice suivit ce conseil, et, pr√©sentant un napol√©on √† la vivandi√®re, la pria de se payer.\n",
      "\n",
      "Il avait alors un souci plus s√©rieux; c'√©taient les lettres de sa tante, qui exigeait absolument qu'il v√Ænt reprendre son appartement au palais Sanseverina, ou que du moins il v√Ænt la voir quelquefois. L√† Fabrice √©tait certain d'entendre parler des f√™tes splendides donn√©es par le marquis Crescenzi √† l'occasion de son mariage: or, c'est ce qu'il n'√©tait pas s√ªr de pouvoir supporter sans se donner en spectacle.\n",
      "\n",
      "Trois jours apr√®s le d√©part de P√©p√©, il fut bien √©tonn√© de recevoir une lettre √©norme ferm√©e avec une tresse de soie comme du temps de Louis XIV, et adress√©e √† Son Excellence r√©v√©rendissime monseigneur Fabrice del Dongo, premier grand-vicaire du dioc√®se de Parme, chanoine, etc.\n",
      "\"Mais, est-ce que je suis encore tout cela?\"se dit-il en riant. L'√©p√Ætre de l'archev√™que Landriani √©tait un chef-d'oeuvre de logique et de clart√©; elle n'avait pas moins de dix-neuf grandes pages, et racontait fort bien tout ce qui s'√©tait pass√© √† Parme √† l'occasion de la mort de Giletti.\n",
      "\n",
      "Il divisa le pain en cinq morceaux et prit le plus petit.\n",
      "\n",
      "Le g√©n√©ral se f√¢cha de plus belle. Pendant ce temps les affaires allaient beaucoup mieux dans la cal√®che.\n",
      "\n",
      "- Mais cela est incroyable, s'√©criaient de vieux courtisans; la faveur de sa tante lui tourne tout √† fait la t√™te... mais, gr√¢ce au ciel, cela ne durera pas; notre souveraine n'aime pas que l'on prenne de ces petits airs de sup√©riorit√©.\n",
      "\n",
      "Question: Quel est le titre du roman ?\n",
      "\n",
      "R√©ponse:\n",
      "üí¨ R√âPONSE:\n",
      "----------------------------------------------------------------------\n",
      "Le roman s'intitule \"La Chartreuse de Parme\".\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Le roman s\\'intitule \"La Chartreuse de Parme\".'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemple 3 - Posez votre propre question\n",
    "ask(\"Quel est le titre du roman ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a64679",
   "metadata": {
    "id": "81a64679"
   },
   "source": [
    "## 11. Changer de provider √† la vol√©e\n",
    "\n",
    "Vous pouvez changer de provider sans recr√©er l'instance RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb5ce77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1763647008270,
     "user": {
      "displayName": "Philippe Helluy",
      "userId": "07790988964707423334"
     },
     "user_tz": -60
    },
    "id": "edb5ce77",
    "outputId": "a1187c6a-9e9b-4d67-9caf-506fc2edfc23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider reconfigur√©: MISTRAL_LARGE | URL: https://api.mistral.ai/v1/chat/completions | Mod√®le: open-mistral-nemo\n",
      "Provider actuel: MISTRAL_LARGE\n",
      "Pour changer: rag.configure_provider('PROVIDER_NAME')\n"
     ]
    }
   ],
   "source": [
    "# Exemple: passer √† un autre provider\n",
    "rag.configure_provider(\"MISTRAL_LARGE\")\n",
    "# rag.configure_provider(\"IRMA_LLMCODE\")\n",
    "# rag.configure_provider(\"PALGANIA_QWEN3\")\n",
    "\n",
    "print(\"Provider actuel:\", rag._provider_cfg.name)\n",
    "print(\"Pour changer: rag.configure_provider('PROVIDER_NAME')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873f185a",
   "metadata": {
    "id": "873f185a"
   },
   "source": [
    "## 12. Sauvegarder des r√©sultats sur Google Drive\n",
    "\n",
    "Exemple d'√©criture de r√©sultats dans Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8b253d",
   "metadata": {
    "id": "bf8b253d"
   },
   "outputs": [],
   "source": [
    "# Cr√©er un fichier de r√©sultats dans Google Drive\n",
    "output_path = '/content/drive/MyDrive/rag_results.txt'\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"R√âSULTATS RAG\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "\n",
    "    # Exemple de sauvegarde d'une Q&A\n",
    "    question = \"Qu'est-ce que le RAG ?\"\n",
    "    results = rag.search(question, top_k=3)\n",
    "    context_docs = [doc for doc, score in results]\n",
    "    response = rag.generate_with_llm(question, context_docs)\n",
    "\n",
    "    f.write(f\"Question: {question}\\n\\n\")\n",
    "    f.write(f\"R√©ponse:\\n{response}\\n\\n\")\n",
    "    f.write(\"Sources:\\n\")\n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        f.write(f\"  [{i}] {doc['source']} (score: {score:.3f})\\n\")\n",
    "\n",
    "print(f\"R√©sultats sauvegard√©s dans: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "415c03f153d74ea0aba0f9faea2c5bea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa2733e4e7c74f64870d9693b341feab",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_42e7952ea1544dc5922439e54f219888",
      "value": "Batches:‚Äá100%"
     }
    },
    "42e7952ea1544dc5922439e54f219888": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "556c450cfe5e4eb58b372dd74573a75d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89e8fd28f2b04b19a09ec2c30695f13f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_da0ef5cb9d244871a95f203aa21f26be",
      "value": "‚Äá56/56‚Äá[00:03&lt;00:00,‚Äá33.59it/s]"
     }
    },
    "576109ea617a4291a6f55a756b4fd302": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63522aba08e94f7f96c972ac093c9951": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "89e8fd28f2b04b19a09ec2c30695f13f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5c208d1d8834a8ab90aa2e93425cd20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_576109ea617a4291a6f55a756b4fd302",
      "max": 56,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_63522aba08e94f7f96c972ac093c9951",
      "value": 56
     }
    },
    "d18209f8cd9048e488cfa57f4643d9ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_415c03f153d74ea0aba0f9faea2c5bea",
       "IPY_MODEL_b5c208d1d8834a8ab90aa2e93425cd20",
       "IPY_MODEL_556c450cfe5e4eb58b372dd74573a75d"
      ],
      "layout": "IPY_MODEL_f6a5547b902f4eb0a4055acf3887dfef"
     }
    },
    "da0ef5cb9d244871a95f203aa21f26be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f6a5547b902f4eb0a4055acf3887dfef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa2733e4e7c74f64870d9693b341feab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
