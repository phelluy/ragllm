"""
Module pour la normalisation et fusion des entit√©s similaires.
"""

import logging
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from config import ENTITY_NORMALIZER_MODEL, ENTITY_SIMILARITY_THRESHOLD

logger = logging.getLogger(__name__)


class EntityNormalizer:
    """
    Normalise et fusionne les entit√©s similaires.
    
    Utilise un mod√®le de sentence-transformers pour encoder les entit√©s et
    d√©tecte les variations/synonymes en utilisant une similarit√© s√©mantique.
    """

    def __init__(self, model_name: str = ENTITY_NORMALIZER_MODEL, threshold: float = ENTITY_SIMILARITY_THRESHOLD):
        """
        Initialise le normaliseur d'entit√©s.
        
        Args:
            model_name: Nom du mod√®le sentence-transformers √† utiliser
            threshold: Seuil de similarit√© (0.0-1.0) pour fusionner deux entit√©s
        """
        logger.info(f"üß† Chargement du mod√®le de normalisation d'entit√©s ({model_name})...")
        self.embed_model = SentenceTransformer(model_name)
        self.threshold = threshold
        self.entity_index = {}      # {canonical_name: [aliases]}
        self.entity_embeddings = {} # {canonical_name: embedding}

    def normalize(self, entity: str) -> str:
        """
        Trouve l'entit√© canonique pour une mention donn√©e.
        
        Processus :
        1. Nettoyage basique (strip, case-insensitive)
        2. V√©rification contre les alias connus (exact match)
        3. Recherche par similarit√© s√©mantique
        4. Cr√©ation d'une nouvelle entit√© canonique si n√©cessaire
        
        Args:
            entity: L'entit√© √† normaliser
            
        Returns:
            L'entit√© canonique (existante ou nouvelle)
        """
        # Nettoyage basique
        entity_clean = entity.strip()
        entity_lower = entity_clean.lower()

        # 1. V√©rifier si c'est un alias connu (exact match, case insensitive)
        for canonical, aliases in self.entity_index.items():
            if entity_lower in [a.lower() for a in aliases]:
                return canonical
        
        # 2. Si pas trouv√©, calculer l'embedding
        try:
            emb = self.embed_model.encode([entity_clean])[0]
        except Exception as e:
            logger.warning(f"Erreur lors du calcul de l'embedding pour '{entity_clean}': {e}")
            # Fallback : cr√©er une nouvelle entit√©
            if entity_clean not in self.entity_index:
                self.entity_index[entity_clean] = [entity_clean]
            return entity_clean

        # 3. Chercher une similarit√© s√©mantique forte avec les entit√©s existantes
        best_match = None
        best_sim = 0.0
        
        if self.entity_embeddings:
            try:
                canonicals = list(self.entity_embeddings.keys())
                embeddings = np.array(list(self.entity_embeddings.values()))
                
                sims = cosine_similarity([emb], embeddings)[0]
                max_idx = np.argmax(sims)
                best_sim = float(sims[max_idx])
                
                if best_sim > self.threshold:
                    best_match = canonicals[max_idx]
            except Exception as e:
                logger.warning(f"Erreur lors de la similarit√© pour '{entity_clean}': {e}")

        if best_match:
            # Fusionner avec la plus similaire
            logger.debug(f"Normalisation : '{entity_clean}' -> '{best_match}' (score: {best_sim:.2f})")
            if entity_clean not in self.entity_index[best_match]:
                self.entity_index[best_match].append(entity_clean)
            return best_match
        else:
            # Cr√©er une nouvelle entit√© canonique
            logger.debug(f"Nouvelle entit√© : '{entity_clean}'")
            self.entity_index[entity_clean] = [entity_clean]
            self.entity_embeddings[entity_clean] = emb
            return entity_clean

    def get_statistics(self) -> dict:
        """
        Retourne des statistiques sur les entit√©s normalis√©es.
        
        Returns:
            Dictionnaire avec nombre d'entit√©s canoniques et d'aliases
        """
        total_aliases = sum(len(aliases) for aliases in self.entity_index.values())
        return {
            "canonical_entities": len(self.entity_index),
            "total_mentions": total_aliases,
            "avg_aliases_per_entity": total_aliases / len(self.entity_index) if self.entity_index else 0
        }

    def log_merges(self):
        """Affiche les regroupements d'entit√©s effectu√©s."""
        logger.info("="*50)
        logger.info("üîÅ REGROUPEMENTS D'ENTIT√âS (Synonymes d√©tect√©s)")
        logger.info("="*50)
        
        found_merges = False
        # Trier par nombre d'alias pour montrer les plus gros groupes en premier
        sorted_entities = sorted(self.entity_index.items(), key=lambda x: len(x[1]), reverse=True)
        
        for canonical, aliases in sorted_entities:
            # Filtrer pour ne garder que les variations (diff√©rentes du canonique)
            variations = [a for a in aliases if a != canonical]
            if variations:
                found_merges = True
                logger.info(f"üîπ [{canonical}] regroupe : {', '.join(variations)}")
        
        if not found_merges:
            logger.info("Aucun regroupement significatif d√©tect√©.")
        logger.info("="*50)
