#!/usr/bin/env python3
"""
D√©monstration simple d'un syst√®me RAG (Retrieval-Augmented Generation)

Ce script illustre les concepts fondamentaux du RAG :
1. Chargement et indexation de documents
2. Cr√©ation d'embeddings
3. Recherche par similarit√©
4. G√©n√©ration de r√©ponse avec un LLM via API REST compatible OpenAI
"""

import os
import glob
import numpy as np
from typing import List, Tuple, Optional
from sentence_transformers import SentenceTransformer
import requests
import json
import urllib3

from llm_providers import get_provider, PROVIDERS

# D√©sactiver les avertissements SSL pour les connexions localhost
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class SimpleRAG:
    """Syst√®me RAG minimaliste pour la d√©monstration, avec s√©lection modulaire de provider LLM."""

    def __init__(
        self,
        data_dir: str = "data",
        provider_name: Optional[str] = None,
        override_model: Optional[str] = None,
        override_url: Optional[str] = None,
        api_key: Optional[str] = None,
    ):
        """Initialise le syst√®me RAG.

        Args:
            data_dir: R√©pertoire contenant les documents markdown.
            provider_name: Nom du provider (cl√© du dictionnaire PROVIDERS). Si None => provider par d√©faut.
            override_model: Permet de forcer un nom de mod√®le diff√©rent.
            override_url: Permet de remplacer l'URL de l'endpoint.
            api_key: Cl√© API explicite (sinon variable d'environnement d√©finie dans le provider).
        """
        self.data_dir = data_dir
        self.documents = []
        self.embeddings = []

        # Mod√®le d'embedding multilingue pour meilleure performance en fran√ßais
        print("Chargement du mod√®le d'embedding...")
        self.embedding_model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")

        # Configuration du provider LLM
        self._provider_cfg = get_provider(
            name=provider_name,
            override_model=override_model,
            override_url=override_url,
            api_key=api_key,
        )
        self.api_url = self._provider_cfg.url
        self.model = self._provider_cfg.model
        self.api_key = self._provider_cfg.api_key_env and os.getenv(self._provider_cfg.api_key_env, "") or ""
        self.use_api = True  # Toujours via API pour la g√©n√©ration

        print(f"Provider s√©lectionn√©: {provider_name or self._provider_cfg.name} | URL: {self.api_url} | Mod√®le: {self.model}")
        if self._provider_cfg.api_key_env:
            if not self.api_key:
                print(
                    f"‚ö†Ô∏è Cl√© API manquante pour {self._provider_cfg.name}. D√©finissez la variable d'environnement '{self._provider_cfg.api_key_env}'."
                )
            else:
                print(f"Cl√© API charg√©e depuis '{self._provider_cfg.api_key_env}'.")
        
    def load_documents(self) -> None:
        """Charge tous les fichiers markdown du r√©pertoire data"""
        print(f"\nChargement des documents depuis {self.data_dir}/...")
        
        md_files = glob.glob(os.path.join(self.data_dir, "*.md"))
        
        for filepath in md_files:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
                # D√©coupage simple par paragraphe
                paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
                
                for para in paragraphs:
                    # Garder tous les paragraphes, m√™me courts
                    self.documents.append({
                        'text': para,
                        'source': os.path.basename(filepath)
                    })
        
        print(f"  ‚Üí {len(self.documents)} chunks charg√©s depuis {len(md_files)} fichiers")
    
    def create_embeddings(self) -> None:
        """G√©n√®re les embeddings pour tous les documents"""
        print("\nCr√©ation des embeddings...")
        
        texts = [doc['text'] for doc in self.documents]
        self.embeddings = self.embedding_model.encode(
            texts,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        print(f"  ‚Üí {len(self.embeddings)} embeddings cr√©√©s (dimension: {self.embeddings.shape[1]})")
    
    def search(self, query: str, top_k: int = 3) -> List[Tuple[dict, float]]:
        """
        Recherche les documents les plus similaires √† la requ√™te
        
        Args:
            query: Question de l'utilisateur
            top_k: Nombre de documents √† retourner
            
        Returns:
            Liste de tuples (document, score de similarit√©)
        """
        # G√©n√©rer l'embedding de la requ√™te
        query_embedding = self.embedding_model.encode(query, convert_to_numpy=True)
        
        # Calculer la similarit√© cosinus avec tous les documents
        similarities = np.dot(self.embeddings, query_embedding) / (
            np.linalg.norm(self.embeddings, axis=1) * np.linalg.norm(query_embedding)
        )
        
        # Obtenir les indices des top-k documents
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        # Retourner les documents avec leurs scores
        results = [
            (self.documents[idx], float(similarities[idx]))
            for idx in top_indices
        ]
        
        return results
    
    def generate_response(self, query: str, context_docs: List[dict]) -> str:
        """
        G√©n√®re une r√©ponse bas√©e sur les documents r√©cup√©r√©s (mode sans LLM)
        
        M√©thode de secours qui retourne simplement le contexte format√©.
        En mode normal, utilisez generate_with_llm() qui appelle l'API REST.
        
        Args:
            query: Question de l'utilisateur
            context_docs: Documents r√©cup√©r√©s
            
        Returns:            R√©ponse g√©n√©r√©e
        """
        # Construction du contexte
        context = "\n\n".join([
            f"[Source: {doc['source']}]\n{doc['text']}"
            for doc in context_docs
        ])
        
        # Retourne simplement le contexte format√©
        response = f"""Bas√© sur les documents suivants, voici des informations pertinentes :

{context}

---
Note : Cette r√©ponse est bas√©e sur les {len(context_docs)} documents les plus pertinents trouv√©s.
Pour une r√©ponse g√©n√©r√©e par LLM, assurez-vous que l'API REST est accessible.
"""
        return response
    
    def configure_provider(
        self,
        provider_name: str,
        override_model: Optional[str] = None,
        override_url: Optional[str] = None,
        api_key: Optional[str] = None,
    ) -> None:
        """Change dynamiquement la configuration du provider LLM.

        Args:
            provider_name: Nom du provider dans PROVIDERS.
            override_model: Surcharge du mod√®le.
            override_url: Surcharge de l'URL.
            api_key: Cl√© API explicite (prioritaire sur variable d'env).
        """
        cfg = get_provider(
            name=provider_name,
            override_model=override_model,
            override_url=override_url,
            api_key=api_key,
        )
        self._provider_cfg = cfg
        self.api_url = cfg.url
        self.model = override_model or cfg.model
        self.api_key = api_key if api_key is not None else (cfg.api_key_env and os.getenv(cfg.api_key_env, "") or "")
        print(f"Provider reconfigur√©: {provider_name} | URL: {self.api_url} | Mod√®le: {self.model}")
        if cfg.api_key_env:
            if not self.api_key:
                print(
                    f"‚ö†Ô∏è Cl√© API manquante pour {cfg.name}. D√©finissez la variable d'environnement '{cfg.api_key_env}'."
                )
            else:
                print(f"Cl√© API charg√©e depuis '{cfg.api_key_env}'.")
    
    def generate_with_llm(self, query: str, context_docs: List[dict]) -> str:
        """
        G√©n√®re une r√©ponse en utilisant l'API REST
        
        Args:
            query: Question de l'utilisateur
            context_docs: Documents r√©cup√©r√©s
            
        Returns:
            R√©ponse g√©n√©r√©e par l'API
        """
        # Construction du contexte
        context = "\n\n".join([doc['text'] for doc in context_docs])
        
        # Construction du prompt
        prompt = f"""Bas√© sur le contexte suivant, r√©ponds √† la question de mani√®re concise et pr√©cise.

Contexte:
{context}

Question: {query}

R√©ponse:"""
        
        # Ajouter l'instruction pour d√©sactiver la r√©flexion si n√©cessaire
        #prompt = prompt + " /no_think"
        
        # Pr√©paration de la requ√™te pour l'API OpenAI-compatible
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            "temperature": 0.3,
            "max_tokens": 2000,
            "top_p": 0.9,
        }
        
        try:
            # Appel √† l'API REST (sans v√©rification SSL pour localhost)
            headers = {"Content-Type": "application/json"}
            if self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"

            verify_ssl = self.api_url.startswith("https://")
            response = requests.post(
                self.api_url,
                json=payload,
                headers=headers,
                verify=verify_ssl,
                timeout=60,
            )
            response.raise_for_status()
            #print("response:", response.json())
            
            # Extraction de la r√©ponse
            result = response.json()
            if "choices" in result and len(result["choices"]) > 0:
                return result["choices"][0]["message"]["content"].strip()
            else:
                return "Erreur : format de r√©ponse inattendu de l'API"
                
        except requests.exceptions.RequestException as e:
            return f"Erreur lors de l'appel √† l'API : {str(e)}"
    
    def interactive_demo(self) -> None:
        """D√©mo interactive permettant de poser des questions"""
        print("\n" + "="*70)
        print("SYST√àME RAG - MODE INTERACTIF")
        print("="*70)
        print("\nPosez vos questions sur le RAG et les LLM.")
        print("Tapez 'quit' ou 'exit' pour quitter.\n")
        
        while True:
            query = input("‚ùì Votre question : ").strip()
            
            if query.lower() in ['quit', 'exit', 'q']:
                print("\nAu revoir !")
                break
            
            if not query:
                continue
            
            # Recherche des documents pertinents
            print("\nüîç Recherche des documents pertinents...")
            results = self.search(query, top_k=20)
            
            print(f"\nüìö Documents trouv√©s (top 3):")
            for i, (doc, score) in enumerate(results, 1):
                print(f"\n  [{i}] Score: {score:.4f} | Source: {doc['source']}")
                print(f"      {doc['text'][:150]}...")
            
            # G√©n√©ration de la r√©ponse
            print("\nüí¨ R√©ponse g√©n√©r√©e:")
            print("-" * 70)
            
            context_docs = [doc for doc, score in results]
            
            # Toujours utiliser l'API pour la g√©n√©ration
            response = self.generate_with_llm(query, context_docs)
            
            print(response)
            print("-" * 70)
            print()


def main():
    """Fonction principale de d√©monstration."""
    print("=" * 70)
    print("D√âMONSTRATION D'UN SYST√àME RAG SIMPLE")
    print("=" * 70)

    # S√©lection optionnelle du provider via variable d'environnement LLM_PROVIDER
    provider_env = os.getenv("LLM_PROVIDER")  # ex: MISTRAL_LARGE
    if provider_env and provider_env not in PROVIDERS:
        print(f"‚ö†Ô∏è Provider '{provider_env}' inconnu. Providers disponibles: {list(PROVIDERS.keys())}")
        provider_env = None

    # Initialisation du syst√®me RAG avec provider modulaire
    rag = SimpleRAG(data_dir="data_big", provider_name=provider_env)
    
    # Chargement et indexation des documents
    rag.load_documents()
    rag.create_embeddings()
    
    # Pour changer dynamiquement le provider pendant la session :
    # rag.configure_provider("MISTRAL_LARGE")
    # rag.configure_provider("PALGANIA_QWEN3", override_model="Qwen3-72B")
    
    # Exemples de requ√™tes
    print("\n" + "="*70)
    print("EXEMPLES DE REQU√äTES")
    print("="*70)
    
    example_queries = [
        "Qu'est-ce que le RAG ?",
        "Comment fonctionnent les embeddings ?",
        "Quelles sont les bases de donn√©es vectorielles ?",
    ]
    
    for query in example_queries:
        print(f"\n‚ùì Question : {query}")
        print("-" * 70)
        
        results = rag.search(query, top_k=3)
        
        print(f"üìö Top 3 documents pertinents :\n")
        for i, (doc, score) in enumerate(results, 1):
            print(f"[{i}] Score: {score:.4f} | Source: {doc['source']}")
            print(f"    {doc['text'][:200]}...")
            print()
    
    # Mode interactif
    print("\n" + "="*70)
    response = input("\nVoulez-vous essayer le mode interactif ? (o/n) : ").strip().lower()
    if response in ['o', 'y', 'oui', 'yes']:
        rag.interactive_demo()
    else:
        print("\nD√©monstration termin√©e. Au revoir !")


if __name__ == "__main__":
    main()
