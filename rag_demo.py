#!/usr/bin/env python3
"""
D√©monstration simple d'un syst√®me RAG (Retrieval-Augmented Generation)

Ce script illustre les concepts fondamentaux du RAG :
1. Chargement et indexation de documents
2. Cr√©ation d'embeddings
3. Recherche par similarit√©
4. G√©n√©ration de r√©ponse avec un LLM
"""

import os
import glob
import numpy as np
from typing import List, Tuple
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch


class SimpleRAG:
    """Syst√®me RAG minimaliste pour la d√©monstration"""
    
    def __init__(self, data_dir: str = "data"):
        """
        Initialise le syst√®me RAG
        
        Args:
            data_dir: R√©pertoire contenant les documents markdown
        """
        self.data_dir = data_dir
        self.documents = []
        self.embeddings = []
        
        # Mod√®le d'embedding l√©ger et performant
        print("Chargement du mod√®le d'embedding...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # LLM l√©ger pour la g√©n√©ration (optionnel, comment√© par d√©faut)
        # D√©commenter si vous avez assez de m√©moire
        self.llm_tokenizer = None
        self.llm_model = None
        
    def load_documents(self) -> None:
        """Charge tous les fichiers markdown du r√©pertoire data"""
        print(f"\nChargement des documents depuis {self.data_dir}/...")
        
        md_files = glob.glob(os.path.join(self.data_dir, "*.md"))
        
        for filepath in md_files:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
                # D√©coupage simple par paragraphe
                paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
                
                for para in paragraphs:
                    # Filtrer les paragraphes trop courts (titres, etc.)
                    if len(para) > 50:
                        self.documents.append({
                            'text': para,
                            'source': os.path.basename(filepath)
                        })
        
        print(f"  ‚Üí {len(self.documents)} chunks charg√©s depuis {len(md_files)} fichiers")
    
    def create_embeddings(self) -> None:
        """G√©n√®re les embeddings pour tous les documents"""
        print("\nCr√©ation des embeddings...")
        
        texts = [doc['text'] for doc in self.documents]
        self.embeddings = self.embedding_model.encode(
            texts,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        print(f"  ‚Üí {len(self.embeddings)} embeddings cr√©√©s (dimension: {self.embeddings.shape[1]})")
    
    def search(self, query: str, top_k: int = 3) -> List[Tuple[dict, float]]:
        """
        Recherche les documents les plus similaires √† la requ√™te
        
        Args:
            query: Question de l'utilisateur
            top_k: Nombre de documents √† retourner
            
        Returns:
            Liste de tuples (document, score de similarit√©)
        """
        # G√©n√©rer l'embedding de la requ√™te
        query_embedding = self.embedding_model.encode(query, convert_to_numpy=True)
        
        # Calculer la similarit√© cosinus avec tous les documents
        similarities = np.dot(self.embeddings, query_embedding) / (
            np.linalg.norm(self.embeddings, axis=1) * np.linalg.norm(query_embedding)
        )
        
        # Obtenir les indices des top-k documents
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        # Retourner les documents avec leurs scores
        results = [
            (self.documents[idx], float(similarities[idx]))
            for idx in top_indices
        ]
        
        return results
    
    def generate_response(self, query: str, context_docs: List[dict]) -> str:
        """
        G√©n√®re une r√©ponse bas√©e sur les documents r√©cup√©r√©s
        
        Pour cette d√©mo, on utilise une g√©n√©ration simple sans LLM.
        En production, on utiliserait un vrai LLM.
        
        Args:
            query: Question de l'utilisateur
            context_docs: Documents r√©cup√©r√©s
            
        Returns:
            R√©ponse g√©n√©r√©e
        """
        # Construction du contexte
        context = "\n\n".join([
            f"[Source: {doc['source']}]\n{doc['text']}"
            for doc in context_docs
        ])
        
        # Pour cette d√©mo, on retourne simplement le contexte format√©
        # En production, on passerait ceci √† un LLM
        response = f"""Bas√© sur les documents suivants, voici des informations pertinentes :

{context}

---
Note : Cette r√©ponse est bas√©e sur les {len(context_docs)} documents les plus pertinents trouv√©s.
Pour une r√©ponse g√©n√©r√©e par LLM, d√©commentez la section d'initialisation du mod√®le de g√©n√©ration.
"""
        return response
    
    def load_llm(self, model_name: str = "TinyLlama/TinyLlama-1.1B-Chat-v1.0") -> None:
        """
        Charge un LLM pour la g√©n√©ration (optionnel)
        
        Args:
            model_name: Nom du mod√®le HuggingFace √† charger
        """
        print(f"\nChargement du LLM {model_name}...")
        print("  (Ceci peut prendre quelques minutes la premi√®re fois)")
        
        self.llm_tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.llm_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            low_cpu_mem_usage=True
        )
        
        print("  ‚Üí LLM charg√© avec succ√®s")
    
    def generate_with_llm(self, query: str, context_docs: List[dict]) -> str:
        """
        G√©n√®re une r√©ponse en utilisant un LLM
        
        Args:
            query: Question de l'utilisateur
            context_docs: Documents r√©cup√©r√©s
            
        Returns:
            R√©ponse g√©n√©r√©e par le LLM
        """
        if self.llm_model is None:
            return self.generate_response(query, context_docs)
        
        # Construction du prompt
        context = "\n\n".join([doc['text'] for doc in context_docs])
        
        prompt = f"""Bas√© sur le contexte suivant, r√©ponds √† la question de mani√®re concise et pr√©cise.

Contexte:
{context}

Question: {query}

R√©ponse:"""
        
        # Tokenisation
        inputs = self.llm_tokenizer(prompt, return_tensors="pt")
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # G√©n√©ration
        outputs = self.llm_model.generate(
            **inputs,
            max_new_tokens=200,
            temperature=0.3,
            do_sample=True,
            top_p=0.9,
            pad_token_id=self.llm_tokenizer.eos_token_id
        )
        
        # D√©codage
        response = self.llm_tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # Extraction de la r√©ponse (apr√®s "R√©ponse:")
        if "R√©ponse:" in response:
            response = response.split("R√©ponse:")[-1].strip()
        
        return response
    
    def interactive_demo(self) -> None:
        """D√©mo interactive permettant de poser des questions"""
        print("\n" + "="*70)
        print("SYST√àME RAG - MODE INTERACTIF")
        print("="*70)
        print("\nPosez vos questions sur le RAG et les LLM.")
        print("Tapez 'quit' ou 'exit' pour quitter.\n")
        
        while True:
            query = input("‚ùì Votre question : ").strip()
            
            if query.lower() in ['quit', 'exit', 'q']:
                print("\nAu revoir !")
                break
            
            if not query:
                continue
            
            # Recherche des documents pertinents
            print("\nüîç Recherche des documents pertinents...")
            results = self.search(query, top_k=3)
            
            print(f"\nüìö Documents trouv√©s (top 3):")
            for i, (doc, score) in enumerate(results, 1):
                print(f"\n  [{i}] Score: {score:.4f} | Source: {doc['source']}")
                print(f"      {doc['text'][:150]}...")
            
            # G√©n√©ration de la r√©ponse
            print("\nüí¨ R√©ponse g√©n√©r√©e:")
            print("-" * 70)
            
            context_docs = [doc for doc, score in results]
            
            if self.llm_model is not None:
                response = self.generate_with_llm(query, context_docs)
            else:
                response = self.generate_response(query, context_docs)
            
            print(response)
            print("-" * 70)
            print()


def main():
    """Fonction principale de d√©monstration"""
    print("="*70)
    print("D√âMONSTRATION D'UN SYST√àME RAG SIMPLE")
    print("="*70)
    
    # Initialisation du syst√®me RAG
    rag = SimpleRAG(data_dir="data")
    
    # Chargement et indexation des documents
    rag.load_documents()
    rag.create_embeddings()
    
    # Optionnel : charger un LLM pour la g√©n√©ration
    # D√©commentez la ligne suivante si vous avez assez de ressources
    # rag.load_llm()
    
    # Exemples de requ√™tes
    print("\n" + "="*70)
    print("EXEMPLES DE REQU√äTES")
    print("="*70)
    
    example_queries = [
        "Qu'est-ce que le RAG ?",
        "Comment fonctionnent les embeddings ?",
        "Quelles sont les bases de donn√©es vectorielles ?",
    ]
    
    for query in example_queries:
        print(f"\n‚ùì Question : {query}")
        print("-" * 70)
        
        results = rag.search(query, top_k=3)
        
        print(f"üìö Top 3 documents pertinents :\n")
        for i, (doc, score) in enumerate(results, 1):
            print(f"[{i}] Score: {score:.4f} | Source: {doc['source']}")
            print(f"    {doc['text'][:200]}...")
            print()
    
    # Mode interactif
    print("\n" + "="*70)
    response = input("\nVoulez-vous essayer le mode interactif ? (o/n) : ").strip().lower()
    if response in ['o', 'y', 'oui', 'yes']:
        rag.interactive_demo()
    else:
        print("\nD√©monstration termin√©e. Au revoir !")


if __name__ == "__main__":
    main()
