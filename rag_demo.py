#!/usr/bin/env python3
"""
D√©monstration simple d'un syst√®me RAG (Retrieval-Augmented Generation)

Ce script illustre les concepts fondamentaux du RAG :
1. Chargement et indexation de documents
2. Cr√©ation d'embeddings
3. Recherche par similarit√©
4. G√©n√©ration de r√©ponse avec un LLM via API REST compatible OpenAI
"""

import os
import glob
import numpy as np
from typing import List, Tuple
from sentence_transformers import SentenceTransformer
import requests
import json
import urllib3

# D√©sactiver les avertissements SSL pour les connexions localhost
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class SimpleRAG:
    """Syst√®me RAG minimaliste pour la d√©monstration"""
    
    def __init__(self, data_dir: str = "data", api_url: str = "http://127.0.0.1:8080/v1/chat/completions"):
        """
        Initialise le syst√®me RAG
        
        Args:
            data_dir: R√©pertoire contenant les documents markdown
            api_url: URL de l'API REST compatible OpenAI
        """
        self.data_dir = data_dir
        self.api_url = api_url
        self.documents = []
        self.embeddings = []
        
        # Mod√®le d'embedding l√©ger et performant
        print("Chargement du mod√®le d'embedding...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # API REST pour la g√©n√©ration (pas de mod√®le local)
        self.use_api = True
        
    def load_documents(self) -> None:
        """Charge tous les fichiers markdown du r√©pertoire data"""
        print(f"\nChargement des documents depuis {self.data_dir}/...")
        
        md_files = glob.glob(os.path.join(self.data_dir, "*.md"))
        
        for filepath in md_files:
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
                # D√©coupage simple par paragraphe
                paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
                
                for para in paragraphs:
                    # Filtrer les paragraphes trop courts (titres, etc.)
                    if len(para) > 50:
                        self.documents.append({
                            'text': para,
                            'source': os.path.basename(filepath)
                        })
        
        print(f"  ‚Üí {len(self.documents)} chunks charg√©s depuis {len(md_files)} fichiers")
    
    def create_embeddings(self) -> None:
        """G√©n√®re les embeddings pour tous les documents"""
        print("\nCr√©ation des embeddings...")
        
        texts = [doc['text'] for doc in self.documents]
        self.embeddings = self.embedding_model.encode(
            texts,
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        print(f"  ‚Üí {len(self.embeddings)} embeddings cr√©√©s (dimension: {self.embeddings.shape[1]})")
    
    def search(self, query: str, top_k: int = 3) -> List[Tuple[dict, float]]:
        """
        Recherche les documents les plus similaires √† la requ√™te
        
        Args:
            query: Question de l'utilisateur
            top_k: Nombre de documents √† retourner
            
        Returns:
            Liste de tuples (document, score de similarit√©)
        """
        # G√©n√©rer l'embedding de la requ√™te
        query_embedding = self.embedding_model.encode(query, convert_to_numpy=True)
        
        # Calculer la similarit√© cosinus avec tous les documents
        similarities = np.dot(self.embeddings, query_embedding) / (
            np.linalg.norm(self.embeddings, axis=1) * np.linalg.norm(query_embedding)
        )
        
        # Obtenir les indices des top-k documents
        top_indices = np.argsort(similarities)[-top_k:][::-1]
        
        # Retourner les documents avec leurs scores
        results = [
            (self.documents[idx], float(similarities[idx]))
            for idx in top_indices
        ]
        
        return results
    
    def generate_response(self, query: str, context_docs: List[dict]) -> str:
        """
        G√©n√®re une r√©ponse bas√©e sur les documents r√©cup√©r√©s (mode sans LLM)
        
        M√©thode de secours qui retourne simplement le contexte format√©.
        En mode normal, utilisez generate_with_llm() qui appelle l'API REST.
        
        Args:
            query: Question de l'utilisateur
            context_docs: Documents r√©cup√©r√©s
            
        Returns:            R√©ponse g√©n√©r√©e
        """
        # Construction du contexte
        context = "\n\n".join([
            f"[Source: {doc['source']}]\n{doc['text']}"
            for doc in context_docs
        ])
        
        # Retourne simplement le contexte format√©
        response = f"""Bas√© sur les documents suivants, voici des informations pertinentes :

{context}

---
Note : Cette r√©ponse est bas√©e sur les {len(context_docs)} documents les plus pertinents trouv√©s.
Pour une r√©ponse g√©n√©r√©e par LLM, assurez-vous que l'API REST est accessible.
"""
        return response
    
    def configure_api(self, api_url: str) -> None:
        """
        Configure l'URL de l'API REST
        
        Args:
            api_url: URL de l'API REST compatible OpenAI
        """
        self.api_url = api_url
        print(f"API configur√©e : {api_url}")
    
    def generate_with_llm(self, query: str, context_docs: List[dict]) -> str:
        """
        G√©n√®re une r√©ponse en utilisant l'API REST
        
        Args:
            query: Question de l'utilisateur
            context_docs: Documents r√©cup√©r√©s
            
        Returns:
            R√©ponse g√©n√©r√©e par l'API
        """
        # Construction du contexte
        context = "\n\n".join([doc['text'] for doc in context_docs])
        
        # Construction du prompt
        prompt = f"""Bas√© sur le contexte suivant, r√©ponds √† la question de mani√®re concise et pr√©cise.

Contexte:
{context}

Question: {query}

R√©ponse:"""
        
        # Ajouter l'instruction pour d√©sactiver la r√©flexion si n√©cessaire
        #prompt = prompt + " /no_think"
        
        # Pr√©paration de la requ√™te pour l'API OpenAI-compatible
        payload = {
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            #"chat_template_kwargs": {"enable_thinking": False},
            "temperature": 0.3,
            "max_tokens": 2000,
            "top_p": 0.9
        }
        
        try:
            # Appel √† l'API REST (sans v√©rification SSL pour localhost)
            response = requests.post(
                self.api_url,
                json=payload,
                headers={"Content-Type": "application/json"},
                verify=False,  # Pas de v√©rification SSL pour localhost
                timeout=30
            )
            response.raise_for_status()
            #print("response:", response.json())
            
            # Extraction de la r√©ponse
            result = response.json()
            if "choices" in result and len(result["choices"]) > 0:
                return result["choices"][0]["message"]["content"].strip()
            else:
                return "Erreur : format de r√©ponse inattendu de l'API"
                
        except requests.exceptions.RequestException as e:
            return f"Erreur lors de l'appel √† l'API : {str(e)}"
    
    def interactive_demo(self) -> None:
        """D√©mo interactive permettant de poser des questions"""
        print("\n" + "="*70)
        print("SYST√àME RAG - MODE INTERACTIF")
        print("="*70)
        print("\nPosez vos questions sur le RAG et les LLM.")
        print("Tapez 'quit' ou 'exit' pour quitter.\n")
        
        while True:
            query = input("‚ùì Votre question : ").strip()
            
            if query.lower() in ['quit', 'exit', 'q']:
                print("\nAu revoir !")
                break
            
            if not query:
                continue
            
            # Recherche des documents pertinents
            print("\nüîç Recherche des documents pertinents...")
            results = self.search(query, top_k=20)
            
            print(f"\nüìö Documents trouv√©s (top 3):")
            for i, (doc, score) in enumerate(results, 1):
                print(f"\n  [{i}] Score: {score:.4f} | Source: {doc['source']}")
                print(f"      {doc['text'][:150]}...")
            
            # G√©n√©ration de la r√©ponse
            print("\nüí¨ R√©ponse g√©n√©r√©e:")
            print("-" * 70)
            
            context_docs = [doc for doc, score in results]
            
            # Toujours utiliser l'API pour la g√©n√©ration
            response = self.generate_with_llm(query, context_docs)
            
            print(response)
            print("-" * 70)
            print()


def main():
    """Fonction principale de d√©monstration"""
    print("="*70)
    print("D√âMONSTRATION D'UN SYST√àME RAG SIMPLE")
    print("="*70)
    
    # Initialisation du syst√®me RAG
    rag = SimpleRAG(data_dir="data_big")
    
    # Chargement et indexation des documents
    rag.load_documents()
    rag.create_embeddings()
    
    # L'API REST est configur√©e par d√©faut (http://127.0.0.1:8080/v1/chat/completions)
    # Pour changer l'URL de l'API, utilisez : rag.configure_api("http://autre-url:port/v1/chat/completions")
    
    # Exemples de requ√™tes
    print("\n" + "="*70)
    print("EXEMPLES DE REQU√äTES")
    print("="*70)
    
    example_queries = [
        "Qu'est-ce que le RAG ?",
        "Comment fonctionnent les embeddings ?",
        "Quelles sont les bases de donn√©es vectorielles ?",
    ]
    
    for query in example_queries:
        print(f"\n‚ùì Question : {query}")
        print("-" * 70)
        
        results = rag.search(query, top_k=3)
        
        print(f"üìö Top 3 documents pertinents :\n")
        for i, (doc, score) in enumerate(results, 1):
            print(f"[{i}] Score: {score:.4f} | Source: {doc['source']}")
            print(f"    {doc['text'][:200]}...")
            print()
    
    # Mode interactif
    print("\n" + "="*70)
    response = input("\nVoulez-vous essayer le mode interactif ? (o/n) : ").strip().lower()
    if response in ['o', 'y', 'oui', 'yes']:
        rag.interactive_demo()
    else:
        print("\nD√©monstration termin√©e. Au revoir !")


if __name__ == "__main__":
    main()
